{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Handling Networks\n",
    "\n",
    "# Task: Create and analyze a directed graph based on conversation data.\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "# Sample DataFrame (conversation data)\n",
    "data = {\n",
    "    \"id\": [\"u1\", \"u2\", \"u3\", \"u4\", \"u5\"],\n",
    "    \"speaker\": [\"Ross\", \"Rachel\", \"Ross\", \"Monica\", \"Chandler\"],\n",
    "    \"reply_to\": [None, \"u1\", \"u2\", \"u3\", \"u4\"],\n",
    "    \"season\": [\"s01\", \"s01\", \"s01\", \"s01\", \"s01\"],\n",
    "    \"episode\": [\"e01\", \"e01\", \"e01\", \"e01\", \"e01\"]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Create a MultiDiGraph\n",
    "G = nx.MultiDiGraph()\n",
    "\n",
    "# Add nodes (speakers) and edges (replies)\n",
    "for _, row in df.iterrows():\n",
    "    G.add_node(row[\"speaker\"])  # Add speaker as a node\n",
    "    if row[\"reply_to\"]:  # Add edges based on replies\n",
    "        reply_speaker = df.loc[df[\"id\"] == row[\"reply_to\"], \"speaker\"].values[0]\n",
    "        G.add_edge(row[\"speaker\"], reply_speaker, season=row[\"season\"], episode=row[\"episode\"])\n",
    "\n",
    "# Print graph properties\n",
    "print(\"Number of nodes:\", G.number_of_nodes())\n",
    "print(\"Number of edges:\", G.number_of_edges())\n",
    "print(\"Nodes:\", G.nodes())\n",
    "print(\"Edges:\", G.edges(data=True))\n",
    "\n",
    "# Alternative representation: Weighted graph\n",
    "# Create a DiGraph with edge weights (number of replies between nodes)\n",
    "weighted_G = nx.DiGraph()\n",
    "for u, v, data in G.edges(data=True):\n",
    "    if weighted_G.has_edge(u, v):\n",
    "        weighted_G[u][v][\"weight\"] += 1\n",
    "    else:\n",
    "        weighted_G.add_edge(u, v, weight=1)\n",
    "\n",
    "print(\"\\nWeighted Graph Edges:\", weighted_G.edges(data=True))\n",
    "\n",
    "# Example 2: Handling Text\n",
    "\n",
    "# Task: Process text data, generate a word-frequency matrix, and compute TF-IDF.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# Sample DataFrame (text data)\n",
    "data = {\n",
    "    \"id\": [\"u1\", \"u2\", \"u3\"],\n",
    "    \"speaker\": [\"Chandler\", \"Ross\", \"Chandler\"],\n",
    "    \"tokens\": [\n",
    "        [\"Could\", \"I\", \"be\", \"more\", \"sarcastic\"],\n",
    "        [\"We\", \"were\", \"on\", \"a\", \"break\"],\n",
    "        [\"Oh\", \"my\", \"God\", \"could\", \"I\", \"be\", \"more\", \"excited\"]\n",
    "    ],\n",
    "    \"episode\": [\"s01_e01\", \"s01_e01\", \"s01_e01\"]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Flatten tokens for Chandler\n",
    "chandler_tokens = df[df[\"speaker\"] == \"Chandler\"][\"tokens\"].sum()\n",
    "\n",
    "# Step A: Create a sorted list of unique tokens\n",
    "unique_tokens = sorted(set(chandler_tokens))\n",
    "print(\"Unique Tokens:\", unique_tokens[:5])\n",
    "\n",
    "# Step B: Create a word-frequency matrix\n",
    "episodes = df[\"episode\"].unique()\n",
    "word_matrix = np.zeros((len(episodes), len(unique_tokens)))\n",
    "\n",
    "for i, ep in enumerate(episodes):\n",
    "    tokens = df[df[\"episode\"] == ep & (df[\"speaker\"] == \"Chandler\")][\"tokens\"].sum()\n",
    "    for token in tokens:\n",
    "        if token in unique_tokens:\n",
    "            word_matrix[i][unique_tokens.index(token)] += 1\n",
    "\n",
    "print(\"\\nWord Matrix Shape:\", word_matrix.shape)\n",
    "print(\"Word Matrix (Chandler):\", word_matrix)\n",
    "\n",
    "# Step C: Convert to TF-IDF\n",
    "vectorizer = TfidfVectorizer(vocabulary=unique_tokens, lowercase=False, tokenizer=lambda x: x, preprocessor=lambda x: x)\n",
    "tfidf_matrix = vectorizer.fit_transform([' '.join(tokens) for tokens in df['tokens']])\n",
    "print(\"\\nTF-IDF Matrix Shape:\", tfidf_matrix.shape)\n",
    "\n",
    "# Example 3: Descriptive Statistics and Data Summarization\n",
    "\n",
    "# Task: Summarize a dataset of TV show episodes and explore character interactions.\n",
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {\n",
    "    \"character\": [\"Ross\", \"Rachel\", \"Monica\", \"Chandler\", \"Phoebe\", \"Joey\"],\n",
    "    \"lines\": [1200, 1100, 950, 1050, 870, 1000],\n",
    "    \"scenes\": [80, 75, 60, 70, 50, 65]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculate basic statistics\n",
    "print(\"Summary Statistics:\")\n",
    "print(df.describe())\n",
    "\n",
    "# Add a new column: Average lines per scene\n",
    "df[\"lines_per_scene\"] = df[\"lines\"] / df[\"scenes\"]\n",
    "print(\"\\nLines Per Scene:\")\n",
    "print(df)\n",
    "\n",
    "# Identify the character with the most lines\n",
    "most_lines = df.loc[df[\"lines\"].idxmax(), \"character\"]\n",
    "print(f\"\\nCharacter with the most lines: {most_lines}\")\n",
    "\n",
    "# Visualize lines per character\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.bar(df[\"character\"], df[\"lines\"])\n",
    "plt.title(\"Lines Per Character\")\n",
    "plt.xlabel(\"Character\")\n",
    "plt.ylabel(\"Lines\")\n",
    "plt.show()\n",
    "\n",
    "# Example 4: Advanced Graph Analysis\n",
    "\n",
    "# Task: Analyze character connections in a show using network centrality metrics.\n",
    "import networkx as nx\n",
    "\n",
    "# Create a directed graph with sample connections\n",
    "G = nx.DiGraph()\n",
    "\n",
    "edges = [\n",
    "    (\"Ross\", \"Rachel\"), (\"Rachel\", \"Ross\"),\n",
    "    (\"Chandler\", \"Joey\"), (\"Joey\", \"Chandler\"),\n",
    "    (\"Monica\", \"Phoebe\"), (\"Phoebe\", \"Monica\"),\n",
    "    (\"Ross\", \"Chandler\"), (\"Rachel\", \"Monica\")\n",
    "]\n",
    "\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "# Calculate centrality measures\n",
    "print(\"Degree Centrality:\", nx.degree_centrality(G))\n",
    "print(\"Betweenness Centrality:\", nx.betweenness_centrality(G))\n",
    "print(\"Closeness Centrality:\", nx.closeness_centrality(G))\n",
    "\n",
    "# Visualize the graph\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pos = nx.spring_layout(G)  # Layout for visualization\n",
    "nx.draw(G, pos, with_labels=True, node_size=1500, node_color=\"lightblue\", arrowsize=20)\n",
    "plt.title(\"Character Interaction Graph\")\n",
    "plt.show()\n",
    "\n",
    "# Example 5: Regression Analysis for Length of Utterances\n",
    "\n",
    "# Task: Investigate the relationship between seasons and utterance lengths.\n",
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {\n",
    "    \"season\": [1, 2, 3, 4, 5],\n",
    "    \"avg_length\": [50, 55, 60, 58, 65]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Add a constant for regression\n",
    "X = sm.add_constant(df[\"season\"])\n",
    "y = df[\"avg_length\"]\n",
    "\n",
    "# Fit a linear regression model\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Display the summary\n",
    "print(model.summary())\n",
    "\n",
    "# Interpret the coefficients\n",
    "intercept = model.params[\"const\"]\n",
    "slope = model.params[\"season\"]\n",
    "print(f\"Intercept: {intercept}, Slope: {slope}\")\n",
    "\n",
    "# Example 6: Text Sentiment Analysis\n",
    "\n",
    "# Task: Perform sentiment analysis on a dataset of dialogue lines.\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Sample DataFrame of dialogue\n",
    "data = {\n",
    "    \"character\": [\"Ross\", \"Rachel\", \"Chandler\", \"Monica\"],\n",
    "    \"dialogue\": [\n",
    "        \"We were on a break!\",\n",
    "        \"It's like all my life everyone has always told me, 'You're a shoe!'\",\n",
    "        \"Could I BE wearing any more clothes?\",\n",
    "        \"Welcome to the real world. It sucks. Youâ€™re gonna love it!\"\n",
    "    ]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Analyze sentiment\n",
    "df[\"polarity\"] = df[\"dialogue\"].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "df[\"subjectivity\"] = df[\"dialogue\"].apply(lambda x: TextBlob(x).sentiment.subjectivity)\n",
    "\n",
    "print(df)\n",
    "\n",
    "# Visualize sentiment\n",
    "import seaborn as sns\n",
    "\n",
    "sns.scatterplot(data=df, x=\"polarity\", y=\"subjectivity\", hue=\"character\")\n",
    "plt.title(\"Sentiment Analysis of Character Dialogues\")\n",
    "plt.show()\n",
    "\n",
    "# Example 7: Unsupervised Learning with Clustering\n",
    "\n",
    "# Task: Group characters based on their interaction statistics.\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "# Sample DataFrame of character interactions\n",
    "data = {\n",
    "    \"character\": [\"Ross\", \"Rachel\", \"Monica\", \"Chandler\", \"Phoebe\", \"Joey\"],\n",
    "    \"lines_spoken\": [1200, 1100, 950, 1050, 870, 1000],\n",
    "    \"interactions\": [300, 280, 260, 270, 250, 290]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Prepare data for clustering\n",
    "X = df[[\"lines_spoken\", \"interactions\"]]\n",
    "\n",
    "# Apply KMeans clustering\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "df[\"cluster\"] = kmeans.fit_predict(X)\n",
    "\n",
    "print(df)\n",
    "\n",
    "# Visualize clusters\n",
    "sns.scatterplot(data=df, x=\"lines_spoken\", y=\"interactions\", hue=\"cluster\", style=\"character\", s=100)\n",
    "plt.title(\"Character Clustering\")\n",
    "plt.show()\n",
    "\n",
    "# Tricks and Examples for Pandas DataFrame Manipulation\n",
    "\n",
    "# Example 8: Basic DataFrame Operations\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {\n",
    "    \"Name\": [\"Alice\", \"Bob\", \"Charlie\", \"David\"],\n",
    "    \"Age\": [25, 30, 35, 40],\n",
    "    \"City\": [\"New York\", \"Los Angeles\", \"Chicago\", \"Houston\"]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Select specific columns\n",
    "print(\"Selecting Columns:\")\n",
    "print(df[\"Name\"])\n",
    "\n",
    "# Filtering rows\n",
    "print(\"\\nFiltering Rows:\")\n",
    "print(df[df[\"Age\"] > 30])\n",
    "\n",
    "# Adding a new column\n",
    "df[\"Age in Months\"] = df[\"Age\"] * 12\n",
    "print(\"\\nNew Column:\")\n",
    "print(df)\n",
    "\n",
    "# Renaming columns\n",
    "df.rename(columns={\"City\": \"Location\"}, inplace=True)\n",
    "print(\"\\nRenamed Columns:\")\n",
    "print(df)\n",
    "\n",
    "# Example 9: Grouping and Aggregations\n",
    "\n",
    "data = {\n",
    "    \"Team\": [\"A\", \"B\", \"A\", \"B\"],\n",
    "    \"Score\": [10, 15, 20, 25],\n",
    "    \"Player\": [\"Alice\", \"Bob\", \"Charlie\", \"David\"]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Group by team and calculate sum of scores\n",
    "grouped = df.groupby(\"Team\")[\"Score\"].sum()\n",
    "print(\"\\nGrouped Data:\")\n",
    "print(grouped)\n",
    "\n",
    "# Example 10: Merging DataFrames\n",
    "\n",
    "left = pd.DataFrame({\"ID\": [1, 2, 3], \"Name\": [\"Alice\", \"Bob\", \"Charlie\"]})\n",
    "right = pd.DataFrame({\"ID\": [1, 2, 4], \"City\": [\"New York\", \"Los Angeles\", \"Chicago\"]})\n",
    "\n",
    "# Inner join\n",
    "merged = pd.merge(left, right, on=\"ID\", how=\"inner\")\n",
    "print(\"\\nMerged DataFrame:\")\n",
    "print(merged)\n",
    "\n",
    "\n",
    "# Tricks and Examples for Pandas DataFrame Manipulation\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Example 1: Basic DataFrame Operations\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {\n",
    "    \"Name\": [\"Alice\", \"Bob\", \"Charlie\", \"David\"],\n",
    "    \"Age\": [25, 30, 35, 40],\n",
    "    \"City\": [\"New York\", \"Los Angeles\", \"Chicago\", \"Houston\"]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Select specific columns\n",
    "print(\"Selecting Columns:\")\n",
    "print(df[\"Name\"])\n",
    "\n",
    "# Filtering rows\n",
    "print(\"\\nFiltering Rows:\")\n",
    "print(df[df[\"Age\"] > 30])\n",
    "\n",
    "# Adding a new column\n",
    "df[\"Age in Months\"] = df[\"Age\"] * 12\n",
    "print(\"\\nNew Column:\")\n",
    "print(df)\n",
    "\n",
    "# Renaming columns\n",
    "df.rename(columns={\"City\": \"Location\"}, inplace=True)\n",
    "print(\"\\nRenamed Columns:\")\n",
    "print(df)\n",
    "\n",
    "# Example 2: Grouping and Aggregations\n",
    "\n",
    "data = {\n",
    "    \"Team\": [\"A\", \"B\", \"A\", \"B\"],\n",
    "    \"Score\": [10, 15, 20, 25],\n",
    "    \"Player\": [\"Alice\", \"Bob\", \"Charlie\", \"David\"]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Group by team and calculate sum of scores\n",
    "grouped = df.groupby(\"Team\")[\"Score\"].sum()\n",
    "print(\"\\nGrouped Data:\")\n",
    "print(grouped)\n",
    "\n",
    "# Example 3: Merging DataFrames\n",
    "\n",
    "left = pd.DataFrame({\"ID\": [1, 2, 3], \"Name\": [\"Alice\", \"Bob\", \"Charlie\"]})\n",
    "right = pd.DataFrame({\"ID\": [1, 2, 4], \"City\": [\"New York\", \"Los Angeles\", \"Chicago\"]})\n",
    "\n",
    "# Inner join\n",
    "merged = pd.merge(left, right, on=\"ID\", how=\"inner\")\n",
    "print(\"\\nMerged DataFrame:\")\n",
    "print(merged)\n",
    "\n",
    "# Example 4: Pivot Tables\n",
    "\n",
    "data = {\n",
    "    \"Date\": [\"2023-01-01\", \"2023-01-02\", \"2023-01-01\", \"2023-01-02\"],\n",
    "    \"Category\": [\"A\", \"A\", \"B\", \"B\"],\n",
    "    \"Value\": [10, 20, 30, 40]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "pivot = df.pivot_table(index=\"Date\", columns=\"Category\", values=\"Value\", aggfunc=\"sum\")\n",
    "print(\"\\nPivot Table:\")\n",
    "print(pivot)\n",
    "\n",
    "# Example 5: Handling Missing Data\n",
    "\n",
    "data = {\n",
    "    \"Name\": [\"Alice\", \"Bob\", \"Charlie\"],\n",
    "    \"Age\": [25, None, 35],\n",
    "    \"City\": [\"New York\", \"Los Angeles\", None]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Fill missing values\n",
    "df.fillna({\"Age\": df[\"Age\"].mean(), \"City\": \"Unknown\"}, inplace=True)\n",
    "print(\"\\nFilled Missing Values:\")\n",
    "print(df)\n",
    "\n",
    "# Drop rows with missing values\n",
    "dropped_df = df.dropna()\n",
    "print(\"\\nDropped Rows with Missing Values:\")\n",
    "print(dropped_df)\n",
    "\n",
    "# Example 6: Sorting Data\n",
    "\n",
    "data = {\n",
    "    \"Name\": [\"Charlie\", \"Alice\", \"Bob\"],\n",
    "    \"Age\": [35, 25, 30],\n",
    "    \"Score\": [90, 80, 85]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Sort by a single column\n",
    "df.sort_values(by=\"Age\", inplace=True)\n",
    "print(\"\\nSorted by Age:\")\n",
    "print(df)\n",
    "\n",
    "# Sort by multiple columns\n",
    "df.sort_values(by=[\"Score\", \"Age\"], ascending=[False, True], inplace=True)\n",
    "print(\"\\nSorted by Score and Age:\")\n",
    "print(df)\n",
    "\n",
    "# Example 7: Apply and Lambda Functions\n",
    "\n",
    "data = {\n",
    "    \"Name\": [\"Alice\", \"Bob\", \"Charlie\"],\n",
    "    \"Score\": [85, 90, 88]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Apply a custom function\n",
    "df[\"Grade\"] = df[\"Score\"].apply(lambda x: \"A\" if x >= 90 else \"B\")\n",
    "print(\"\\nApplied Custom Function:\")\n",
    "print(df)\n",
    "\n",
    "# Example 8: String Operations\n",
    "\n",
    "data = {\n",
    "    \"Name\": [\"Alice\", \"Bob\", \"Charlie\"],\n",
    "    \"Email\": [\"alice@example.com\", \"bob@example.com\", \"charlie@example.com\"]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Extract domain from email\n",
    "df[\"Domain\"] = df[\"Email\"].str.split(\"@\").str[1]\n",
    "print(\"\\nExtracted Domain:\")\n",
    "print(df)\n",
    "\n",
    "# Example 9: Working with Dates\n",
    "\n",
    "data = {\n",
    "    \"Event\": [\"Meeting\", \"Conference\", \"Webinar\"],\n",
    "    \"Date\": [\"2023-01-01\", \"2023-02-15\", \"2023-03-10\"]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Convert to datetime\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "print(\"\\nConverted to Datetime:\")\n",
    "print(df)\n",
    "\n",
    "# Calculate days until the event\n",
    "df[\"Days Until\"] = (df[\"Date\"] - pd.Timestamp(\"2023-01-01\")).dt.days\n",
    "print(\"\\nDays Until Event:\")\n",
    "print(df)\n",
    "\n",
    "# Example 10: Reshaping Data\n",
    "\n",
    "data = {\n",
    "    \"Name\": [\"Alice\", \"Bob\", \"Charlie\"],\n",
    "    \"Subject\": [\"Math\", \"Science\", \"English\"],\n",
    "    \"Score\": [85, 90, 88]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Melt the DataFrame\n",
    "melted = pd.melt(df, id_vars=\"Name\", value_vars=[\"Subject\", \"Score\"], var_name=\"Attribute\", value_name=\"Value\")\n",
    "print(\"\\nMelted DataFrame:\")\n",
    "print(melted)\n",
    "\n",
    "# Example 11: MultiIndex DataFrames\n",
    "\n",
    "data = {\n",
    "    \"Region\": [\"North\", \"North\", \"South\", \"South\"],\n",
    "    \"Product\": [\"A\", \"B\", \"A\", \"B\"],\n",
    "    \"Sales\": [100, 150, 200, 250]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Set MultiIndex\n",
    "df.set_index([\"Region\", \"Product\"], inplace=True)\n",
    "print(\"\\nMultiIndex DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Access data in MultiIndex\n",
    "data_north = df.loc[\"North\"]\n",
    "print(\"\\nData for North Region:\")\n",
    "print(data_north)\n",
    "\n",
    "# Example 12: Combining DataFrames\n",
    "\n",
    "# Concatenate DataFrames\n",
    "df1 = pd.DataFrame({\"Name\": [\"Alice\", \"Bob\"], \"Age\": [25, 30]})\n",
    "df2 = pd.DataFrame({\"Name\": [\"Charlie\", \"David\"], \"Age\": [35, 40]})\n",
    "combined = pd.concat([df1, df2], ignore_index=True)\n",
    "print(\"\\nConcatenated DataFrame:\")\n",
    "print(combined)\n",
    "\n",
    "# Example 13: Sampling Data\n",
    "\n",
    "data = {\n",
    "    \"Name\": [\"Alice\", \"Bob\", \"Charlie\", \"David\"],\n",
    "    \"Score\": [85, 90, 88, 92]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Random sample of rows\n",
    "sampled = df.sample(n=2, random_state=42)\n",
    "print(\"\\nRandom Sample of Rows:\")\n",
    "print(sampled)\n",
    "\n",
    "# Example 14: Exploding Lists into Rows\n",
    "\n",
    "data = {\n",
    "    \"Name\": [\"Alice\", \"Bob\"],\n",
    "    \"Hobbies\": [[\"Reading\", \"Cycling\"], [\"Swimming\", \"Hiking\"]]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Explode the lists into rows\n",
    "df_exploded = df.explode(\"Hobbies\")\n",
    "print(\"\\nExploded DataFrame:\")\n",
    "print(df_exploded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matching\n",
    "\n",
    "# Examples of Matching by Propensity Score or Features in Common\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "# Sample DataFrame\n",
    "np.random.seed(42)\n",
    "data = {\n",
    "    \"ID\": range(1, 21),\n",
    "    \"Applied_Treatment\": np.random.choice([0, 1], size=20, p=[0.5, 0.5]),\n",
    "    \"Feature1\": np.random.normal(0, 1, 20),\n",
    "    \"Feature2\": np.random.normal(5, 2, 20),\n",
    "    \"Propensity_Score\": np.random.uniform(0, 1, 20)\n",
    "}\n",
    "data_df = pd.DataFrame(data)\n",
    "\n",
    "# Separate into treatment and control groups\n",
    "treatment_df = data_df[data_df[\"Applied_Treatment\"] == 1]\n",
    "control_df = data_df[data_df[\"Applied_Treatment\"] == 0]\n",
    "\n",
    "print(f\"There are {treatment_df.shape[0]} samples in the treated group.\")\n",
    "print(f\"There are {control_df.shape[0]} samples in the control group.\")\n",
    "\n",
    "# Function to calculate similarity based on Euclidean distance of features\n",
    "def get_feature_similarity(row1, row2, feature_columns):\n",
    "    return 1 / (1 + np.linalg.norm(row1[feature_columns] - row2[feature_columns]))\n",
    "\n",
    "# Matching using Propensity Score\n",
    "G = nx.Graph()\n",
    "\n",
    "for treatment_id, treatment_row in treatment_df.iterrows():\n",
    "    for control_id, control_row in control_df.iterrows():\n",
    "        similarity = 1 - np.abs(treatment_row[\"Propensity_Score\"] - control_row[\"Propensity_Score\"])\n",
    "        if similarity > 0.8:  # Adjust threshold as needed\n",
    "            G.add_weighted_edges_from([(treatment_row[\"ID\"], control_row[\"ID\"], similarity)])\n",
    "        \n",
    "matching = nx.max_weight_matching(G, maxcardinality=True)\n",
    "\n",
    "print(\"\\nMatching by Propensity Score:\")\n",
    "print(matching)\n",
    "\n",
    "# Matching using Features (Feature1 and Feature2)\n",
    "feature_columns = [\"Feature1\", \"Feature2\"]\n",
    "G = nx.Graph()\n",
    "\n",
    "for treatment_id, treatment_row in treatment_df.iterrows():\n",
    "    for control_id, control_row in control_df.iterrows():\n",
    "        similarity = get_feature_similarity(treatment_row, control_row, feature_columns)\n",
    "        if similarity > 0.7:  # Adjust threshold as needed\n",
    "            G.add_weighted_edges_from([(treatment_row[\"ID\"], control_row[\"ID\"], similarity)])\n",
    "\n",
    "feature_matching = nx.max_weight_matching(G, maxcardinality=True)\n",
    "\n",
    "print(\"\\nMatching by Features:\")\n",
    "print(feature_matching)\n",
    "\n",
    "# Additional Matching Methods\n",
    "\n",
    "# 1. Exact Matching on Features (e.g., categorical variables)\n",
    "def exact_match(df1, df2, column):\n",
    "    matches = []\n",
    "    for _, row1 in df1.iterrows():\n",
    "        for _, row2 in df2.iterrows():\n",
    "            if row1[column] == row2[column]:\n",
    "                matches.append((row1[\"ID\"], row2[\"ID\"]))\n",
    "    return matches\n",
    "\n",
    "# Create a new column for exact matching demonstration\n",
    "data_df[\"Group\"] = np.random.choice([\"A\", \"B\"], size=20)\n",
    "\n",
    "treatment_df = data_df[data_df[\"Applied_Treatment\"] == 1]\n",
    "control_df = data_df[data_df[\"Applied_Treatment\"] == 0]\n",
    "\n",
    "exact_matches = exact_match(treatment_df, control_df, \"Group\")\n",
    "print(\"\\nExact Matches:\")\n",
    "print(exact_matches)\n",
    "\n",
    "# 2. Nearest Neighbor Matching\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "nn = NearestNeighbors(n_neighbors=1, metric=\"euclidean\")\n",
    "X_treatment = treatment_df[feature_columns].values\n",
    "X_control = control_df[feature_columns].values\n",
    "\n",
    "nn.fit(X_control)\n",
    "_, indices = nn.kneighbors(X_treatment)\n",
    "\n",
    "nearest_neighbor_matches = [(treatment_df.iloc[i][\"ID\"], control_df.iloc[idx][\"ID\"]) for i, idx in enumerate(indices.flatten())]\n",
    "\n",
    "print(\"\\nNearest Neighbor Matches:\")\n",
    "print(nearest_neighbor_matches)\n",
    "\n",
    "# 3. Propensity Score Stratification\n",
    "bins = np.linspace(0, 1, 5)\n",
    "data_df[\"Propensity_Bin\"] = pd.cut(data_df[\"Propensity_Score\"], bins=bins)\n",
    "\n",
    "stratified_groups = data_df.groupby([\"Propensity_Bin\", \"Applied_Treatment\"])\n",
    "print(\"\\nStratified Groups:\")\n",
    "for name, group in stratified_groups:\n",
    "    print(f\"Group: {name}\")\n",
    "    print(group)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
